{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.neighbors import NearestNeighbors as SklearnNN\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import  SAGEConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import statistics\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amostras no df_sampled: 20757\n",
      "Amostras nos embeddings: 20757\n"
     ]
    }
   ],
   "source": [
    "node_features = np.load(\"../../pre_process/embeddings.npy\")\n",
    "df_sampled = pd.read_pickle(\"../../pre_process/df_sampled.pkl\")\n",
    "\n",
    "node_features = torch.tensor(node_features, dtype=torch.float32).clone().detach()\n",
    "print(\"Amostras no df_sampled:\", df_sampled.shape[0])\n",
    "print(\"Amostras nos embeddings:\", node_features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafo intra-projeto criado (bidirecional):\n",
      "  - Nós: torch.Size([1596, 384])\n",
      "  - Arestas: torch.Size([2, 3657])\n"
     ]
    }
   ],
   "source": [
    "BERT_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PROJECT_NAMES = {\n",
    "    36: \"MULE\",\n",
    "    43: \"EVG\",\n",
    "    12: \"TIMOB\",\n",
    "    4:  \"MESOS\"\n",
    "}\n",
    "PROJECT_ID = 4 # Projeto de teste MULE(36), EVG(43), TIMOB(12), MESOS(4)\n",
    "project_name = PROJECT_NAMES.get(PROJECT_ID, f\"project_{PROJECT_ID}\")\n",
    "VERSAO_MODELO = f\"sage_within_{project_name}\"\n",
    "\n",
    "# Carregar dados\n",
    "node_features_all = np.load(\"../../pre_process/embeddings.npy\")\n",
    "df_sampled = pd.read_pickle(\"../../pre_process/df_sampled.pkl\")\n",
    "\n",
    "# Filtrar apenas o projeto desejado\n",
    "df_project = df_sampled[df_sampled[\"Project_ID\"] == PROJECT_ID].copy().reset_index(drop=True)\n",
    "node_features = np.vstack(df_project[\"BERT_Embedding\"])\n",
    "\n",
    "node_features = torch.tensor(node_features, dtype=torch.float32).clone().detach()\n",
    "\n",
    "# Criar arestas kNN\n",
    "k = 4\n",
    "sknn = SklearnNN(n_neighbors=k, metric=\"cosine\").fit(node_features)\n",
    "_, indices = sknn.kneighbors(node_features)\n",
    "\n",
    "edges = []\n",
    "for i in range(len(df_project)):\n",
    "    for j in range(1, k):  # Ignora auto-loop\n",
    "        edges.append([i, indices[i, j]])\n",
    "\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).T\n",
    "\n",
    "# Tornar bidirecional\n",
    "edge_index_bi = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
    "\n",
    "# Remover duplicatas se desejar (opcional, recomendado)\n",
    "sorted_edge_index = torch.stack([\n",
    "    torch.min(edge_index_bi[0], edge_index_bi[1]),\n",
    "    torch.max(edge_index_bi[0], edge_index_bi[1])\n",
    "], dim=0)\n",
    "\n",
    "edge_index_unique = torch.unique(sorted_edge_index, dim=1)\n",
    "\n",
    "# Criar o objeto PyG\n",
    "data = Data(\n",
    "    x=node_features,\n",
    "    edge_index=edge_index_unique\n",
    ")\n",
    "\n",
    "print(\"Grafo intra-projeto criado (bidirecional):\")\n",
    "print(\"  - Nós:\", data.x.shape)\n",
    "print(\"  - Arestas:\", data.edge_index.shape)\n",
    "\n",
    "\n",
    "# Função para filtrar subgrafos isolados\n",
    "def filter_edges_by_split(edge_index, train_idx, val_idx, test_idx):\n",
    "    allowed_train_val = set(train_idx.tolist() + val_idx.tolist())\n",
    "    allowed_test = set(test_idx.tolist())\n",
    "\n",
    "    filtered_edges = []\n",
    "\n",
    "    for src, dst in edge_index.T.tolist():\n",
    "        if (src in allowed_train_val and dst in allowed_train_val) or \\\n",
    "           (src in allowed_test and dst in allowed_test):\n",
    "            filtered_edges.append([src, dst])\n",
    "\n",
    "    return torch.tensor(filtered_edges, dtype=torch.long).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 10 runs (base seed=42)\n",
      "\n",
      "--- Run 1/10 (seed=42) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10 Epoch 58/200 - Train Loss: 0.000086 - Val MSE: 0.001412\n",
      "Early stopping (run 1) after epoch 58. Best Val MSE: 0.001298\n",
      "\n",
      "--- Run 2/10 (seed=43) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2/10 Epoch 54/200 - Train Loss: 0.000076 - Val MSE: 0.002193\n",
      "Early stopping (run 2) after epoch 54. Best Val MSE: 0.002147\n",
      "\n",
      "--- Run 3/10 (seed=44) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3/10 Epoch 52/200 - Train Loss: 0.000155 - Val MSE: 0.002592\n",
      "Early stopping (run 3) after epoch 52. Best Val MSE: 0.002466\n",
      "\n",
      "--- Run 4/10 (seed=45) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4/10 Epoch 53/200 - Train Loss: 0.000056 - Val MSE: 0.002396\n",
      "Early stopping (run 4) after epoch 53. Best Val MSE: 0.002223\n",
      "\n",
      "--- Run 5/10 (seed=46) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5/10 Epoch 79/200 - Train Loss: 0.000052 - Val MSE: 0.001287\n",
      "Early stopping (run 5) after epoch 79. Best Val MSE: 0.001227\n",
      "\n",
      "--- Run 6/10 (seed=47) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6/10 Epoch 52/200 - Train Loss: 0.000142 - Val MSE: 0.002526\n",
      "Early stopping (run 6) after epoch 52. Best Val MSE: 0.002188\n",
      "\n",
      "--- Run 7/10 (seed=48) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 7/10 Epoch 35/200 - Train Loss: 0.000390 - Val MSE: 0.003690\n",
      "Early stopping (run 7) after epoch 35. Best Val MSE: 0.003291\n",
      "\n",
      "--- Run 8/10 (seed=49) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 8/10 Epoch 56/200 - Train Loss: 0.000087 - Val MSE: 0.002412\n",
      "Early stopping (run 8) after epoch 56. Best Val MSE: 0.002358\n",
      "\n",
      "--- Run 9/10 (seed=50) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9/10 Epoch 52/200 - Train Loss: 0.000193 - Val MSE: 0.002036\n",
      "Early stopping (run 9) after epoch 52. Best Val MSE: 0.001919\n",
      "\n",
      "--- Run 10/10 (seed=51) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10/10 Epoch 47/200 - Train Loss: 0.000283 - Val MSE: 0.002848\n",
      "Early stopping (run 10) after epoch 47. Best Val MSE: 0.002833\n",
      "\n",
      "All runs finished.\n",
      "Summary saved to: runs_results_MESOS/sage_within_MESOS_summary_10runs.json\n",
      "MAE mean (real scale): 1.436916402313345 ± 0.09076944207922437\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURAÇÃO DE RUNS \n",
    "N_RUNS = 10\n",
    "OUT_DIR = f\"runs_results_{project_name}\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Hiperparâmetros \n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "DROPOUT = 0\n",
    "PATIENCE = 5\n",
    "NORMALIZE_TYPE = \"minmax\"  # \"minmax\", \"standard\", or None\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "class SAGEModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = torch.relu(self.conv2(x, edge_index))\n",
    "        return self.lin(x).view(-1)\n",
    "\n",
    "# Salvar backup do edge_index original (não filtrado)\n",
    "edge_index_orig = data.edge_index.cpu().clone().detach()\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def prepare_split_and_loaders(seed, edge_index_backup):\n",
    "    \"\"\"\n",
    "    Restaura edge_index, cria split (70/15/15) com seed fornecida,\n",
    "    ajusta scaler somente com train, define data.y escalonado, e cria NeighborLoaders.\n",
    "    Retorna: scaler_fit, train_loader, val_loader, test_loader, train_idx, val_idx, test_idx\n",
    "    \"\"\"\n",
    "    # Restaura grafo global\n",
    "    data.edge_index = edge_index_backup.clone().to(data.x.device)\n",
    "\n",
    "    # Indices\n",
    "    all_indices = np.arange(data.num_nodes)\n",
    "\n",
    "    # Split \n",
    "    train_idx, temp_idx = train_test_split(all_indices, test_size=0.3, random_state=seed)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=seed)\n",
    "\n",
    "    # Ajustar scaler \n",
    "    sp_train = df_project.loc[train_idx, \"Story_Point\"].fillna(0).values.reshape(-1, 1)\n",
    "\n",
    "    if NORMALIZE_TYPE == \"minmax\":\n",
    "        scaler_local = MinMaxScaler().fit(sp_train)\n",
    "    elif NORMALIZE_TYPE == \"standard\":\n",
    "        scaler_local = StandardScaler().fit(sp_train)\n",
    "    else:\n",
    "        scaler_local = None\n",
    "\n",
    "    # Transformar todos os rótulos (train+val+test) com scaler ajustado no train\n",
    "    sp_all = df_project[\"Story_Point\"].fillna(0).values.reshape(-1, 1)\n",
    "    if scaler_local is not None:\n",
    "        sp_all_scaled = scaler_local.transform(sp_all).ravel()\n",
    "    else:\n",
    "        sp_all_scaled = sp_all.ravel()\n",
    "\n",
    "    data.y = torch.tensor(sp_all_scaled, dtype=torch.float32)\n",
    "\n",
    "    # Filtrar arestas entre grupos diferentes\n",
    "    data.edge_index = filter_edges_by_split(\n",
    "        edge_index=data.edge_index,\n",
    "        train_idx=train_idx,\n",
    "        val_idx=val_idx,\n",
    "        test_idx=test_idx\n",
    "    ).to(data.x.device)\n",
    "\n",
    "    # Criar loaders\n",
    "    train_loader = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=[3,3],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        input_nodes=torch.tensor(train_idx),\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_loader = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=[3,3],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        input_nodes=torch.tensor(val_idx),\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_loader = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=[3,3],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        input_nodes=torch.tensor(test_idx),\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return scaler_local, train_loader, val_loader, test_loader, train_idx, val_idx, test_idx\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            outputs = model(batch).cpu().numpy()\n",
    "            targets = batch.y.cpu().numpy()\n",
    "\n",
    "            loss = criterion(torch.tensor(outputs), torch.tensor(targets))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions.extend(outputs)\n",
    "            actuals.extend(targets)\n",
    "\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    mdae = median_absolute_error(actuals, predictions)\n",
    "    errors = np.abs(np.array(predictions) - np.array(actuals))\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        within_50 = np.where(np.array(actuals) != 0, errors <= (0.50 * np.array(actuals)), errors == 0)\n",
    "    pred_50 = float(np.mean(within_50))\n",
    "\n",
    "    return total_loss / max(1, len(dataloader)), float(mae), float(mse), float(mdae), float(pred_50)\n",
    "\n",
    "# Loop principal de runs \n",
    "all_run_results = []\n",
    "\n",
    "print(f\"Starting {N_RUNS} runs (base seed={SEED})\")\n",
    "for run_id in range(N_RUNS):\n",
    "    run_seed = SEED + run_id\n",
    "    set_all_seeds(run_seed)\n",
    "\n",
    "    # preparar dados / loaders (restaurando edge_index original)\n",
    "    scaler_run, train_loader, val_loader, test_loader, train_idx, val_idx, test_idx = prepare_split_and_loaders(run_seed, edge_index_orig)\n",
    "\n",
    "    # criar modelo novo para a run\n",
    "    in_channels = data.x.size(1)\n",
    "    hidden_dim = 64\n",
    "    model = SAGEModel(in_channels, hidden_dim).to(DEVICE)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=(PATIENCE // 2)+1, factor=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_mse_history = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n--- Run {run_id+1}/{N_RUNS} (seed={run_seed}) ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch)\n",
    "            targets = batch.y.to(DEVICE)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / max(1, len(train_loader))\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # validação\n",
    "        _, _, val_mse, *_ = evaluate_model(model, val_loader, criterion)\n",
    "        val_mse_history.append(val_mse)\n",
    "\n",
    "        print(f\"\\rRun {run_id+1}/{N_RUNS} Epoch {epoch+1}/{EPOCHS} - Train Loss: {avg_train_loss:.6f} - Val MSE: {val_mse:.6f}\", end=\"\", flush=True)\n",
    "\n",
    "        # early stopping logic\n",
    "        if val_mse < best_val_loss:\n",
    "            best_val_loss = val_mse\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        scheduler.step(val_mse)\n",
    "\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"\\nEarly stopping (run {run_id+1}) after epoch {epoch+1}. Best Val MSE: {best_val_loss:.6f}\")\n",
    "            break\n",
    "\n",
    "    # restaura melhor modelo\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # avaliação final no teste\n",
    "    _, mae_scaled, mse_scaled, mdae_scaled, pred50_scaled = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "    # converter métricas para escala real \n",
    "    if scaler_run is not None:\n",
    "        if NORMALIZE_TYPE == \"minmax\":\n",
    "            SP_MIN, SP_MAX = df_project[\"Story_Point\"].min(), df_project[\"Story_Point\"].max()\n",
    "            scale_factor = SP_MAX - SP_MIN\n",
    "            if scale_factor == 0:\n",
    "                scale_factor = 1.0\n",
    "            test_metrics_real = {\n",
    "                \"MAE\": mae_scaled * scale_factor,\n",
    "                \"MSE\": mse_scaled * (scale_factor ** 2),\n",
    "                \"MdAE\": mdae_scaled * scale_factor,\n",
    "                \"Pred(50)\": pred50_scaled\n",
    "            }\n",
    "        elif NORMALIZE_TYPE == \"standard\":\n",
    "            std_dev_real = df_project[\"Story_Point\"].std()\n",
    "            if std_dev_real == 0:\n",
    "                std_dev_real = 1.0\n",
    "            test_metrics_real = {\n",
    "                \"MAE\": mae_scaled * std_dev_real,\n",
    "                \"MSE\": mse_scaled * (std_dev_real ** 2),\n",
    "                \"MdAE\": mdae_scaled * std_dev_real,\n",
    "                \"Pred(50)\": pred50_scaled\n",
    "            }\n",
    "        else:\n",
    "            test_metrics_real = {\"MAE\": mae_scaled, \"MSE\": mse_scaled, \"MdAE\": mdae_scaled, \"Pred(50)\": pred50_scaled}\n",
    "    else:\n",
    "        test_metrics_real = {\"MAE\": mae_scaled, \"MSE\": mse_scaled, \"MdAE\": mdae_scaled, \"Pred(50)\": pred50_scaled}\n",
    "\n",
    "    # salvar resultados da run\n",
    "    run_result = {\n",
    "        \"run_id\": run_id + 1,\n",
    "        \"seed\": run_seed,\n",
    "        \"elapsed_sec\": elapsed,\n",
    "        \"best_val_mse\": float(best_val_loss),\n",
    "        \"epochs_trained\": epoch+1,\n",
    "        \"train_loss_history\": train_losses,\n",
    "        \"val_mse_history\": val_mse_history,\n",
    "        \"test_metrics_scaled\": {\"MAE\": float(mae_scaled), \"MSE\": float(mse_scaled), \"MdAE\": float(mdae_scaled), \"Pred(50)\": float(pred50_scaled)},\n",
    "        \"test_metrics_real\": {k: float(v) for k, v in test_metrics_real.items()}\n",
    "    }\n",
    "\n",
    "    # salvar arquivo json e pesos\n",
    "    json_path = os.path.join(OUT_DIR, f\"{VERSAO_MODELO}_run{run_id+1}_seed{run_seed}.json\")\n",
    "    torch_path = os.path.join(OUT_DIR, f\"{VERSAO_MODELO}_run{run_id+1}_seed{run_seed}.pt\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(run_result, f, indent=4)\n",
    "    torch.save(best_model_state, torch_path)\n",
    "\n",
    "    all_run_results.append(run_result)\n",
    "\n",
    "    # liberar cache GPU\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# --- Consolidação final ---\n",
    "maes = [r[\"test_metrics_real\"][\"MAE\"] for r in all_run_results]\n",
    "mses = [r[\"test_metrics_real\"][\"MSE\"] for r in all_run_results]\n",
    "mdaes = [r[\"test_metrics_real\"][\"MdAE\"] for r in all_run_results]\n",
    "pred50s = [r[\"test_metrics_real\"][\"Pred(50)\"] for r in all_run_results]\n",
    "\n",
    "summary = {\n",
    "    \"n_runs\": N_RUNS,\n",
    "    \"seed_base\": SEED,\n",
    "    \"MAE_mean\": float(statistics.mean(maes)),\n",
    "    \"MAE_std\": float(statistics.pstdev(maes)),\n",
    "    \"MSE_mean\": float(statistics.mean(mses)),\n",
    "    \"MSE_std\": float(statistics.pstdev(mses)),\n",
    "    \"MdAE_mean\": float(statistics.mean(mdaes)),\n",
    "    \"MdAE_std\": float(statistics.pstdev(mdaes)),\n",
    "    \"Pred50_mean\": float(statistics.mean(pred50s)),\n",
    "    \"Pred50_std\": float(statistics.pstdev(pred50s)),\n",
    "    \"per_run_files\": [os.path.basename(f\"{VERSAO_MODELO}_run{r['run_id']}_seed{r['seed']}.json\") for r in all_run_results]\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(OUT_DIR, f\"{VERSAO_MODELO}_summary_{N_RUNS}runs.json\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump({\"summary\": summary, \"runs\": all_run_results}, f, indent=4)\n",
    "\n",
    "print(\"\\nAll runs finished.\")\n",
    "print(\"Summary saved to:\", summary_path)\n",
    "print(\"MAE mean (real scale):\", summary[\"MAE_mean\"], \"±\", summary[\"MAE_std\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Docker)",
   "language": "python",
   "name": "python_docker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
