{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento Story Points e Descrição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import mysql.connector\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando dados do banco...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_126269/1041495142.py:111: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34691 issues carregadas. Aplicando Normalização SP 3...\n",
      " 20757 issues restantes após normalização.\n",
      " Dados normalizados salvos em 'issues_sp_filtered.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Função para limpar texto preservando código\n",
    "def clean_text_preserving_tags(text):\n",
    "    \"\"\" \n",
    "    Limpa texto removendo ruído, mas preservando tags XML/HTML para garantir a estrutura.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    # Remover emojis que podem interferir no modelo\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "\n",
    "    # Remover apenas tags HTML que não carregam significado (ex.: <b>, <i>), mas manter XML/HTML úteis\n",
    "    text = re.sub(r'</?(?:b|i|p|span|div|br|strong|em)>', '', text)\n",
    "\n",
    "    # Manter estrutura de código markdown (blocos ``` e ~~~)\n",
    "    text = re.sub(r'(```.*?```|~~~.*?~~~)', r'\\1', text, flags=re.DOTALL)\n",
    "\n",
    "    # Manter inline code entre <code>...</code>\n",
    "    text = re.sub(r'<code>(.*?)</code>', r' `\\1` ', text)\n",
    "\n",
    "    # Manter todas as outras tags XML/HTML, incluindo <localStatus>IN_SERVICE</localStatus>\n",
    "    text = re.sub(r'(<[^>]+>)', r' \\1 ', text)  # Adiciona espaço ao redor para facilitar NLP\n",
    "\n",
    "    # Substituir ponto e vírgula por dois-pontos para evitar problemas de separação no CSV\n",
    "    text = text.replace(\";\", \":\")\n",
    "\n",
    "    # Manter estrutura de código e caracteres relevantes para análise\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?_(){}\\[\\]<>:;=#/\\*\\+\\-]\", \"\", text)\n",
    "\n",
    "    # Substituir múltiplos espaços por um único, mas manter estrutura XML\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Função para aplicar a Normalização SP 3\n",
    "def normalize_sp3(df):\n",
    "    \"\"\" Aplica a Normalização SP 3 no dataset de issues. \"\"\"\n",
    "    \n",
    "    # Aplicar filtros nos Story Points\n",
    "    df_filtered = df[\n",
    "        (df[\"Story_Point\"].notnull()) & \n",
    "        (df[\"Story_Point\"] >= 0) & (df[\"Story_Point\"] <= 100) & \n",
    "        ((df[\"Story_Point\"] % 1 == 0) | (df[\"Story_Point\"] == 0.5))\n",
    "    ]\n",
    "\n",
    "    # Contar total de issues antes dos filtros por projeto\n",
    "    df_total_before = df.groupby([\"Project_ID\", \"Project_Name\"])[\"Issue_ID\"].count().reset_index()\n",
    "    df_total_before.rename(columns={\"Issue_ID\": \"Total_Issues_Before\"}, inplace=True)\n",
    "\n",
    "    # Remover issues com frequência <= 5 por projeto\n",
    "    issue_counts = df_filtered.groupby([\"Project_ID\", \"Story_Point\"]).Issue_ID.transform(\"count\")\n",
    "    df_filtered = df_filtered[issue_counts > 5]\n",
    "\n",
    "    # Contar total de issues após os filtros por projeto\n",
    "    df_summary = df_filtered.groupby([\"Project_ID\", \"Project_Name\"]).agg(\n",
    "        Total_Issues_After=(\"Issue_ID\", \"count\")\n",
    "    ).reset_index()\n",
    "\n",
    "    # Mesclar para calcular impacto da remoção\n",
    "    df_final = df_total_before.merge(df_summary, on=[\"Project_ID\", \"Project_Name\"], how=\"right\")\n",
    "    df_final[\"Porcentagem_Removida\"] = ((df_final[\"Total_Issues_Before\"] - df_final[\"Total_Issues_After\"]) / df_final[\"Total_Issues_Before\"]) * 100\n",
    "\n",
    "    # Remover projetos com menos de 100 issues ou impacto > 10%\n",
    "    df_final = df_final[(df_final[\"Total_Issues_After\"] >= 100) & (df_final[\"Porcentagem_Removida\"] <= 10)]\n",
    "\n",
    "    # Filtrar novamente as issues que pertencem aos projetos que sobraram\n",
    "    df_filtered = df_filtered[df_filtered[\"Project_ID\"].isin(df_final[\"Project_ID\"])]\n",
    "\n",
    "    # Aplicar limpeza preservando código\n",
    "    df_filtered[\"Title\"] = df_filtered[\"Title\"].apply(clean_text_preserving_tags)\n",
    "    df_filtered[\"Description\"] = df_filtered[\"Description\"].apply(clean_text_preserving_tags)\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# Conectar ao MySQL e buscar os dados\n",
    "def fetch_issues_from_db():\n",
    "    \"\"\" Conecta ao MySQL, executa a consulta e retorna um DataFrame com os dados. \"\"\"\n",
    "    \n",
    "    # Configuração do MySQL (substitua com suas credenciais)\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"tawos-db\",\n",
    "        user=\"root\",\n",
    "        password=\"rootpassword\",\n",
    "        database=\"tawosdb\"\n",
    "    )\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        r.ID AS Repository_ID,\n",
    "        r.Name AS Repository_Name,\n",
    "        p.ID AS Project_ID, \n",
    "        p.Name AS Project_Name,\n",
    "        i.ID AS Issue_ID,\n",
    "        i.Title,\n",
    "        i.Description,\n",
    "        i.Assignee_ID,\n",
    "        i.Story_Point\n",
    "    FROM Issue i\n",
    "    JOIN Project p ON i.Project_ID = p.ID\n",
    "    JOIN Repository r ON p.Repository_ID = r.ID\n",
    "    WHERE i.Story_Point BETWEEN 0 AND 100\n",
    "        AND i.Resolution_Date IS NOT NULL\n",
    "        AND LOWER(TRIM(i.Resolution)) IN ('fixed', 'done', 'completed')\n",
    "        AND LOWER(TRIM(i.Status)) IN ('closed', 'done', 'resolved', 'complete')\n",
    "        AND i.Assignee_ID IS NOT NULL\n",
    "        AND TIMESTAMPDIFF(DAY, i.Creation_Date, i.Resolution_Date) BETWEEN 0 AND 180\n",
    "        AND i.Description IS NOT NULL AND TRIM(i.Description) <> ''\n",
    "    \"\"\"\n",
    "\n",
    "    # Executar a consulta e carregar os dados\n",
    "    df = pd.read_sql(query, conn)\n",
    "\n",
    "    # Fechar conexão\n",
    "    conn.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Executar pipeline completo\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Buscando dados do banco...\")\n",
    "    df_issues = fetch_issues_from_db()\n",
    "\n",
    "    print(f\"{len(df_issues)} issues carregadas. Aplicando Normalização SP 3...\")\n",
    "    df_normalized = normalize_sp3(df_issues)\n",
    "\n",
    "    print(f\" {len(df_normalized)} issues restantes após normalização.\")\n",
    "    \n",
    "    # Salvar o resultado em um arquivo CSV\n",
    "    df_normalized.to_csv(\"issues_sp_filtered.csv\", index=False)\n",
    "    df_issues.to_csv(\"issues_sp_unfiltered.csv\", index=False)\n",
    "\n",
    "    print(\" Dados normalizados salvos em 'issues_sp_filtered.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando Developers_ars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando subconjunto de issues...\n",
      "Buscando datas de início das issues...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_285833/71804283.py:51: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando número de issues resolvidas antes de cada issue...\n",
      "Fazendo merge com o CSV original...\n",
      "Visualizando os primeiros resultados:\n",
      "   Repository_ID Repository_Name  Project_ID    Project_Name  Issue_ID  \\\n",
      "0              2        Sonatype           3  Sonatype Nexus      5004   \n",
      "1              2        Sonatype           3  Sonatype Nexus      5049   \n",
      "2              2        Sonatype           3  Sonatype Nexus      5129   \n",
      "3              2        Sonatype           3  Sonatype Nexus      5203   \n",
      "4              2        Sonatype           3  Sonatype Nexus      5268   \n",
      "\n",
      "                                               Title  \\\n",
      "0  Conan integration in 3.22.0 does not handle He...   \n",
      "1  Problem proxying NuGet packages hosted by GitH...   \n",
      "2  NullPointer appears if run cleanup for hosted ...   \n",
      "3             [R format] regression test full format   \n",
      "4  [Helm Features] Cleanup policy facet should be...   \n",
      "\n",
      "                                         Description  Assignee_ID  \\\n",
      "0  I am unable to upload header only conan pacakg...          518   \n",
      "1  * ,create a new NuGet (proxy) repository ** na...          462   \n",
      "2  *Steps to reproduce:* 1. Create repository usi...          500   \n",
      "3  Regression test full R format when all tickets...          500   \n",
      "4  Accepted criteria: * User should be able to se...          518   \n",
      "\n",
      "   Story_Point    In_Progress_Date     Resolution_Date       Creation_Date  \\\n",
      "0          0.0 2020-03-30 19:44:27 2020-04-10 13:46:45 2020-03-30 19:44:27   \n",
      "1          1.0 2020-03-05 07:56:25 2020-03-26 13:04:06 2020-03-05 07:56:25   \n",
      "2          1.0 2019-12-24 10:16:23 2020-01-13 12:51:08 2019-12-24 10:16:23   \n",
      "3          3.0 2019-10-15 14:05:16 2019-10-28 12:43:26 2019-10-15 14:05:16   \n",
      "4          2.0 2019-08-27 15:14:11 2019-10-01 15:34:04 2019-08-27 15:14:11   \n",
      "\n",
      "   Creator_ID  Developer_AR  \n",
      "0       646.0             1  \n",
      "1       663.0             0  \n",
      "2       500.0             1  \n",
      "3       464.0             0  \n",
      "4       518.0             0  \n",
      "Arquivo salvo com sucesso: issues_sp_filtered_with_developer_ar.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "def fetch_issue_start_dates():\n",
    "    \"\"\"Busca a data mais próxima do início do desenvolvimento de cada issue e calcula o número de issues resolvidas antes dessa data.\"\"\"\n",
    "    \n",
    "    # Conectar ao banco de dados\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"tawos-db\",\n",
    "        user=\"root\",\n",
    "        password=\"rootpassword\",\n",
    "        database=\"tawosdb\"\n",
    "    )\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            i.ID AS Issue_ID,\n",
    "            i.Creation_Date,\n",
    "            i.Creator_ID,\n",
    "            COALESCE(\n",
    "                MIN(cl.Creation_Date),  -- Data da primeira mudança para \"To Do\", \"In Progress\" ou \"To Develop\"\n",
    "                s.First_Sprint_Activated_Date, -- Data de ativação da primeira Sprint associada à issue\n",
    "                i.Creation_Date -- Caso não tenha nenhuma das anteriores, usa a data de criação da issue\n",
    "            ) AS In_Progress_Date,\n",
    "            i.Resolution_Date,\n",
    "            i.Assignee_ID\n",
    "        FROM Issue i\n",
    "        LEFT JOIN Change_Log cl \n",
    "            ON cl.Issue_ID = i.ID \n",
    "            AND LOWER(TRIM(cl.Field)) = 'status'\n",
    "            AND LOWER(TRIM(cl.To_String)) IN ('to do', 'in progress', 'to develop')\n",
    "        LEFT JOIN (\n",
    "            SELECT \n",
    "                il.Sprint_ID, \n",
    "                il.ID AS Issue_ID,\n",
    "                MIN(s.Activated_Date) AS First_Sprint_Activated_Date\n",
    "            FROM Issue il\n",
    "            JOIN Sprint s ON il.Sprint_ID = s.ID\n",
    "            WHERE s.Activated_Date IS NOT NULL\n",
    "            GROUP BY il.ID\n",
    "        ) s ON s.Issue_ID = i.ID\n",
    "        WHERE i.Story_Point BETWEEN 0 AND 100\n",
    "            AND i.Resolution_Date IS NOT NULL\n",
    "            AND LOWER(TRIM(i.Resolution)) IN ('fixed', 'done', 'completed')\n",
    "            AND LOWER(TRIM(i.Status)) IN ('closed', 'done', 'resolved', 'complete')\n",
    "            AND TIMESTAMPDIFF(DAY, i.Creation_Date, i.Resolution_Date) BETWEEN 0 AND 180\n",
    "            AND i.Description IS NOT NULL\n",
    "        GROUP BY i.ID, s.First_Sprint_Activated_Date, i.Creation_Date;\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def compute_developer_ar(df, filtered_issues):\n",
    "    \"\"\"Calcula quantas issues o desenvolvedor resolveu antes da issue atual, garantindo ordenação correta por resolução.\n",
    "       Considera apenas issues que estejam no subconjunto de `filtered_issues`.\n",
    "    \"\"\"\n",
    "    \n",
    "    df[\"In_Progress_Date\"] = pd.to_datetime(df[\"In_Progress_Date\"])\n",
    "    df[\"Resolution_Date\"] = pd.to_datetime(df[\"Resolution_Date\"])\n",
    "\n",
    "    # Filtrar apenas as issues do subconjunto do CSV, mantendo apenas as colunas necessárias\n",
    "    df_filtered = df[df[\"Issue_ID\"].isin(filtered_issues[\"Issue_ID\"])].copy()\n",
    "\n",
    "    developer_ars = {}\n",
    "\n",
    "    for assignee in df_filtered[\"Assignee_ID\"].unique():\n",
    "        df_dev = df_filtered[df_filtered[\"Assignee_ID\"] == assignee].sort_values(\"Resolution_Date\")\n",
    "\n",
    "        resolved_counts = []\n",
    "        for i, row in df_dev.iterrows():\n",
    "            # Contar apenas issues resolvidas antes da atual\n",
    "            count = df_dev[\n",
    "                (df_dev[\"Resolution_Date\"] < row[\"In_Progress_Date\"]) & \n",
    "                (df_dev[\"Resolution_Date\"] >= row[\"In_Progress_Date\"] - pd.DateOffset(years=1))\n",
    "            ].shape[0]\n",
    "            resolved_counts.append(count)\n",
    "\n",
    "        developer_ars.update(dict(zip(df_dev[\"Issue_ID\"], resolved_counts)))\n",
    "\n",
    "    df_filtered.loc[:, \"Developer_AR\"] = df_filtered[\"Issue_ID\"].map(developer_ars)\n",
    "\n",
    "    # Selecionar apenas as colunas relevantes\n",
    "    return df_filtered[[\"Issue_ID\", \"In_Progress_Date\", \"Resolution_Date\", \"Creation_Date\", \"Creator_ID\", \"Developer_AR\"]]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Carregando subconjunto de issues...\")\n",
    "    df_filtered_issues = pd.read_csv(\"issues_sp_filtered.csv\")\n",
    "\n",
    "    print(\"Buscando datas de início das issues...\")\n",
    "    df_issues = fetch_issue_start_dates()\n",
    "\n",
    "    print(\"Calculando número de issues resolvidas antes de cada issue...\")\n",
    "    df_issues_filtered = compute_developer_ar(df_issues, df_filtered_issues)\n",
    "\n",
    "    print(\"Fazendo merge com o CSV original...\")\n",
    "    df_final = df_filtered_issues.merge(df_issues_filtered, on=\"Issue_ID\", how=\"left\")\n",
    "\n",
    "    print(\"Visualizando os primeiros resultados:\")\n",
    "    print(df_final.head())\n",
    "\n",
    "    # Salvar para análise posterior\n",
    "    df_final.to_csv(\"issues_sp_filtered_with_developer_ar.csv\", index=False)\n",
    "    print(\"Arquivo salvo com sucesso: issues_sp_filtered_with_developer_ar.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculo da Reputação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando CSV com Developer_AR...\n",
      "Calculando reputação temporal com base em Resolution_Date (últimos 12 meses)...\n",
      "Amostra dos resultados:\n",
      "   Issue_ID  Assignee_ID  Developer_AR  Reputation_t\n",
      "0      5004          518             1           0.5\n",
      "1      5049          462             0           0.0\n",
      "2      5129          500             1           0.0\n",
      "3      5203          500             0           0.0\n",
      "4      5268          518             0           0.0\n",
      "Arquivo final salvo: issues_sp_filtered_with_reputation_t.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "def calculate_temporal_reputation(df):\n",
    "    \"\"\"Calcula reputação temporal usando resolução recente (últimos 12 meses) como critério de janela.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"In_Progress_Date\"] = pd.to_datetime(df[\"In_Progress_Date\"])\n",
    "    df[\"Resolution_Date\"] = pd.to_datetime(df[\"Resolution_Date\"])\n",
    "\n",
    "    reputations = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        assignee = row[\"Assignee_ID\"]\n",
    "        in_progress_date = row[\"In_Progress_Date\"]\n",
    "        start_window = in_progress_date - timedelta(days=365)\n",
    "\n",
    "        # Janela de issues resolvidas nos 12 meses anteriores\n",
    "        window_df = df[\n",
    "            (df[\"Resolution_Date\"] >= start_window) &\n",
    "            (df[\"Resolution_Date\"] < in_progress_date)\n",
    "        ]\n",
    "\n",
    "        # Issues criadas por esse desenvolvedor\n",
    "        opened = window_df[window_df[\"Creator_ID\"] == assignee]\n",
    "\n",
    "        # Destas, quantas ele também resolveu\n",
    "        opened_and_fixed = opened[opened[\"Assignee_ID\"] == assignee]\n",
    "\n",
    "        reputation_t = len(opened_and_fixed) / (len(opened) + 1)\n",
    "        reputations.append(reputation_t)\n",
    "\n",
    "    df[\"Reputation_t\"] = reputations\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Carregando CSV com Developer_AR...\")\n",
    "    df = pd.read_csv(\"issues_sp_filtered_with_developer_ar.csv\")\n",
    "\n",
    "    print(\"Calculando reputação temporal com base em Resolution_Date (últimos 12 meses)...\")\n",
    "    df_result = calculate_temporal_reputation(df)\n",
    "\n",
    "    print(\"Amostra dos resultados:\")\n",
    "    print(df_result[[\"Issue_ID\", \"Assignee_ID\", \"Developer_AR\", \"Reputation_t\"]].head())\n",
    "\n",
    "    df_result.to_csv(\"issues_sp_filtered_with_reputation_t.csv\", index=False)\n",
    "    print(\"Arquivo final salvo: issues_sp_filtered_with_reputation_t.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando CSV base...\n",
      "Buscando dados do banco...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_285833/3069545764.py:28: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mesclando dados com CSV...\n",
      "Visualizando amostra:\n",
      "   Issue_ID Issue_Type  Num_Comentarios  Comentarios_Assignee\n",
      "0      5004        Bug                4                     0\n",
      "1      5049        Bug                0                     0\n",
      "2      5129        Bug                0                     0\n",
      "3      5203       Task                0                     0\n",
      "4      5268      Story                5                     3\n",
      "Novo CSV salvo: issues_sp_filtered_with_comments.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "def fetch_issue_type_and_comments(issue_ids):\n",
    "    \"\"\"Consulta o banco para retornar tipo da issue e número de comentários por issue + assignee.\"\"\"\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"tawos-db\",\n",
    "        user=\"root\",\n",
    "        password=\"rootpassword\",\n",
    "        database=\"tawosdb\"\n",
    "    )\n",
    "\n",
    "    # Converter lista para string de IDs\n",
    "    issue_id_list = \",\".join(map(str, issue_ids))\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            i.ID AS Issue_ID,\n",
    "            i.Type AS Issue_Type,\n",
    "            COUNT(c_all.ID) AS Num_Comentarios,\n",
    "            COUNT(CASE WHEN c_all.Author_ID = i.Assignee_ID THEN 1 END) AS Comentarios_Assignee\n",
    "        FROM Issue i\n",
    "        LEFT JOIN Comment c_all ON c_all.Issue_ID = i.ID\n",
    "        WHERE i.ID IN ({issue_id_list})\n",
    "        GROUP BY i.ID\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Carregando CSV base...\")\n",
    "    df_base = pd.read_csv(\"issues_sp_filtered_with_reputation_t.csv\")\n",
    "\n",
    "    print(\"Buscando dados do banco...\")\n",
    "    issue_ids = df_base[\"Issue_ID\"].tolist()\n",
    "    df_comments = fetch_issue_type_and_comments(issue_ids)\n",
    "\n",
    "    print(\"Mesclando dados com CSV...\")\n",
    "    df_merged = df_base.merge(df_comments, on=\"Issue_ID\", how=\"left\")\n",
    "\n",
    "    print(\"Visualizando amostra:\")\n",
    "    print(df_merged[[\"Issue_ID\", \"Issue_Type\", \"Num_Comentarios\", \"Comentarios_Assignee\"]].head())\n",
    "\n",
    "    df_merged.to_csv(\"issues_sp_filtered_with_comments.csv\", index=False)\n",
    "    print(\"Novo CSV salvo: issues_sp_filtered_with_comments.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar o CSV existente\n",
    "df = pd.read_csv(\"issues_sp_filtered_with_comments.csv\")\n",
    "\n",
    "# Calcular estatísticas por projeto\n",
    "project_stats = df.groupby(\"Project_ID\")[\"Story_Point\"].agg(\n",
    "    SP_Intervalo_Projeto=lambda x: x.max() - x.min(),\n",
    "    Project_SP_Std=\"std\"\n",
    ").reset_index()\n",
    "\n",
    "# Mesclar com o DataFrame original\n",
    "df_enriched = df.merge(project_stats, on=\"Project_ID\", how=\"left\")\n",
    "\n",
    "# Visualizar amostra\n",
    "print(df_enriched[[\"Project_ID\", \"Story_Point\", \"SP_Intervalo_Projeto\", \"Project_SP_Std\"]].head())\n",
    "\n",
    "# Salvar novo CSV\n",
    "df_enriched.to_csv(\"issues_sp_with_project_stats.csv\", index=False)\n",
    "print(\"Novo CSV salvo como 'issues_sp_with_project_stats.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Docker)",
   "language": "python",
   "name": "python_docker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
