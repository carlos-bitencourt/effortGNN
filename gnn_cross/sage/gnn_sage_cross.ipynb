{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.neighbors import NearestNeighbors as SklearnNN\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import  SAGEConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import statistics\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amostras no df_sampled: 20757\n",
      "Amostras nos embeddings: 20757\n"
     ]
    }
   ],
   "source": [
    "node_features = np.load(\"../../pre_process/embeddings.npy\")\n",
    "df_sampled = pd.read_pickle(\"../../pre_process/df_sampled.pkl\")\n",
    "\n",
    "node_features = torch.tensor(node_features, dtype=torch.float32).clone().detach()\n",
    "print(\"Amostras no df_sampled:\", df_sampled.shape[0])\n",
    "print(\"Amostras nos embeddings:\", node_features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de projetos únicos: 28\n",
      "IDs dos projetos: [ 3  4  5  7  8  9 10 11 16 17 18 14 22 25 27 26 23 24 29 30 31 43 34 35\n",
      " 36 13 12 33]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar o dataset processado\n",
    "df_sampled = pd.read_pickle(\"../../pre_process/df_sampled.pkl\")\n",
    "\n",
    "# Contar número de projetos únicos\n",
    "n_projects = df_sampled[\"Project_ID\"].nunique()\n",
    "projects = df_sampled[\"Project_ID\"].unique()\n",
    "\n",
    "print(\"Número de projetos únicos:\", n_projects)\n",
    "print(\"IDs dos projetos:\", projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafo intra-projeto criado (bidirecional):\n",
      "  - Nós: torch.Size([20757, 384])\n",
      "  - Arestas: torch.Size([2, 47915])\n"
     ]
    }
   ],
   "source": [
    "BERT_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# PROJECTS = [13,34,36]  # Projetos de treino: \"*\" para todos exceto o de teste, ou lista explícita [12, 14]\n",
    "PROJECTS = [\"*\"] # proejtos de treino\n",
    "PROJECT_NAMES = { # projetos de teste\n",
    "    36: \"MULE\",\n",
    "    43: \"EVG\",\n",
    "    12: \"TIMOB\",\n",
    "    4:  \"MESOS\"\n",
    "}\n",
    "PROJECT_ID = 4   # Projeto de teste MULE(36), EVG(43), TIMOB(12), MESOS(4)\n",
    "project_name = PROJECT_NAMES.get(PROJECT_ID, f\"project_{PROJECT_ID}\")\n",
    "VERSAO_MODELO = f\"sage_cross_{project_name}\"\n",
    "\n",
    "# Carregar dados\n",
    "node_features_all = np.load(\"../../pre_process/embeddings.npy\")\n",
    "df_sampled = pd.read_pickle(\"../../pre_process/df_sampled.pkl\")\n",
    "\n",
    "# Interpretar PROJECTS\n",
    "if \"*\" in PROJECTS:\n",
    "    all_projects = df_sampled[\"Project_ID\"].unique()\n",
    "    train_projects = [pid for pid in all_projects if pid != PROJECT_ID]\n",
    "else:\n",
    "    train_projects = PROJECTS\n",
    "\n",
    "# Separar conjuntos de treino e teste\n",
    "df_train = df_sampled[df_sampled[\"Project_ID\"].isin(train_projects)]\n",
    "df_test = df_sampled[df_sampled[\"Project_ID\"] == PROJECT_ID]\n",
    "\n",
    "# Concatenar para manter índice consistente\n",
    "df_all = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
    "node_features = np.vstack(df_all[\"BERT_Embedding\"])\n",
    "node_features = torch.tensor(node_features, dtype=torch.float32).clone().detach()\n",
    "\n",
    "\n",
    "# Criar arestas kNN\n",
    "k = 4\n",
    "sknn = SklearnNN(n_neighbors=k, metric=\"cosine\").fit(node_features)\n",
    "_, indices = sknn.kneighbors(node_features)\n",
    "\n",
    "edges = []\n",
    "for i in range(len(df_all)):\n",
    "    for j in range(1, k):  # Ignora auto-loop\n",
    "        edges.append([i, indices[i, j]])\n",
    "\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).T\n",
    "\n",
    "# Tornar bidirecional\n",
    "edge_index_bi = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
    "\n",
    "# Remover duplicatas\n",
    "sorted_edge_index = torch.stack([\n",
    "    torch.min(edge_index_bi[0], edge_index_bi[1]),\n",
    "    torch.max(edge_index_bi[0], edge_index_bi[1])\n",
    "], dim=0)\n",
    "\n",
    "edge_index_unique = torch.unique(sorted_edge_index, dim=1)\n",
    "\n",
    "# Criar o objeto PyG\n",
    "data = Data(\n",
    "    x=node_features,\n",
    "    edge_index=edge_index_unique\n",
    ")\n",
    "\n",
    "print(\"Grafo intra-projeto criado (bidirecional):\")\n",
    "print(\"  - Nós:\", data.x.shape)\n",
    "print(\"  - Arestas:\", data.edge_index.shape)\n",
    "\n",
    "\n",
    "# Função para filtrar subgrafos isolados\n",
    "def filter_edges_by_split(edge_index, train_idx, val_idx, test_idx):\n",
    "    allowed_train_val = set(train_idx.tolist() + val_idx.tolist())\n",
    "    allowed_test = set(test_idx.tolist())\n",
    "\n",
    "    filtered_edges = []\n",
    "\n",
    "    for src, dst in edge_index.T.tolist():\n",
    "        if (src in allowed_train_val and dst in allowed_train_val) or \\\n",
    "           (src in allowed_test and dst in allowed_test):\n",
    "            filtered_edges.append([src, dst])\n",
    "\n",
    "    return torch.tensor(filtered_edges, dtype=torch.long).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando 10 runs (cross-project) - test project: MESOS (ID 4)\n",
      "\n",
      "--- Run 1/10 (seed=42) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10 Epoch 46/200 - Train Loss: 0.000049 - Val MSE: 0.000235\n",
      "Early stopping (run 1) after epoch 46. Best Val MSE: 0.000227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 2/10 (seed=43) ---\n",
      "Run 2/10 Epoch 74/200 - Train Loss: 0.000033 - Val MSE: 0.000134\n",
      "Early stopping (run 2) after epoch 74. Best Val MSE: 0.000131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 3/10 (seed=44) ---\n",
      "Run 3/10 Epoch 88/200 - Train Loss: 0.000026 - Val MSE: 0.000123\n",
      "Early stopping (run 3) after epoch 88. Best Val MSE: 0.000122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 4/10 (seed=45) ---\n",
      "Run 4/10 Epoch 60/200 - Train Loss: 0.000041 - Val MSE: 0.000160\n",
      "Early stopping (run 4) after epoch 60. Best Val MSE: 0.000158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 5/10 (seed=46) ---\n",
      "Run 5/10 Epoch 63/200 - Train Loss: 0.000038 - Val MSE: 0.000261\n",
      "Early stopping (run 5) after epoch 63. Best Val MSE: 0.000261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 6/10 (seed=47) ---\n",
      "Run 6/10 Epoch 79/200 - Train Loss: 0.000025 - Val MSE: 0.000128\n",
      "Early stopping (run 6) after epoch 79. Best Val MSE: 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 7/10 (seed=48) ---\n",
      "Run 7/10 Epoch 59/200 - Train Loss: 0.000042 - Val MSE: 0.000195\n",
      "Early stopping (run 7) after epoch 59. Best Val MSE: 0.000186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 8/10 (seed=49) ---\n",
      "Run 8/10 Epoch 64/200 - Train Loss: 0.000031 - Val MSE: 0.000204\n",
      "Early stopping (run 8) after epoch 64. Best Val MSE: 0.000202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 9/10 (seed=50) ---\n",
      "Run 9/10 Epoch 49/200 - Train Loss: 0.000043 - Val MSE: 0.000135\n",
      "Early stopping (run 9) after epoch 49. Best Val MSE: 0.000133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 10/10 (seed=51) ---\n",
      "Run 10/10 Epoch 93/200 - Train Loss: 0.000023 - Val MSE: 0.000122\n",
      "Early stopping (run 10) after epoch 93. Best Val MSE: 0.000118\n",
      "\n",
      "All cross-project runs finished.\n",
      "Summary saved to: runs_results_MESOS/sage_cross_MESOS_summary_10runs.json\n",
      "MAE mean (real scale): 0.23887102064478918 ± 0.01871952359269433\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURAÇÃO DE RUNS \n",
    "N_RUNS = 10\n",
    "OUT_DIR = f\"runs_results_{project_name}\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Hiperparâmetros \n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "DROPOUT = 0\n",
    "PATIENCE = 5\n",
    "NORMALIZE_TYPE = \"minmax\"  # \"minmax\", \"standard\", or None\n",
    "SEED = 42\n",
    "\n",
    "# Backup do edge_index original\n",
    "edge_index_orig = data.edge_index.cpu().clone().detach()\n",
    "\n",
    "class SAGEModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = torch.relu(self.conv2(x, edge_index))\n",
    "        return self.lin(x).view(-1)\n",
    "\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def prepare_split_and_loaders_cross(seed, edge_index_backup):\n",
    "    \"\"\"\n",
    "    Para o cenário cross-project: restaura grafo, cria split treino/val apenas entre os nós de treino,\n",
    "    ajusta scaler no train, define data.y e cria NeighborLoader para train/val/test.\n",
    "    Retorna: scaler_local, train_loader, val_loader, test_loader, train_idx_run, val_idx_run, test_idx (fixo)\n",
    "    \"\"\"\n",
    "    # Restaura grafo global original\n",
    "    data.edge_index = edge_index_backup.clone().to(data.x.device)\n",
    "\n",
    "    # train_idx_base contém todos os nós de treino (todos os projetos de treino)\n",
    "    train_idx_base = df_all[df_all[\"Project_ID\"].isin(train_projects)].index.to_numpy()\n",
    "    test_idx = df_all[df_all[\"Project_ID\"] == PROJECT_ID].index.to_numpy()\n",
    "\n",
    "    # Split interno do train -> train/val (usando apenas train_idx_base)\n",
    "    train_idx_run, val_idx_run = train_test_split(train_idx_base, test_size=0.15, random_state=seed)\n",
    "\n",
    "    # Ajustar scaler APENAS com dados de treino (evita leakage)\n",
    "    sp_train = df_all.loc[train_idx_run, \"Story_Point\"].fillna(0).values.reshape(-1, 1)\n",
    "    if NORMALIZE_TYPE == \"minmax\":\n",
    "        scaler_local = MinMaxScaler().fit(sp_train)\n",
    "    elif NORMALIZE_TYPE == \"standard\":\n",
    "        scaler_local = StandardScaler().fit(sp_train)\n",
    "    else:\n",
    "        scaler_local = None\n",
    "\n",
    "    # Transformar todos os rótulos (train+val+test) com scaler ajustado no train\n",
    "    sp_all = df_all[\"Story_Point\"].fillna(0).values.reshape(-1, 1)\n",
    "    if scaler_local is not None:\n",
    "        sp_all_scaled = scaler_local.transform(sp_all).ravel()\n",
    "    else:\n",
    "        sp_all_scaled = sp_all.ravel()\n",
    "\n",
    "    data.y = torch.tensor(sp_all_scaled, dtype=torch.float32)\n",
    "\n",
    "    # Filtrar arestas entre grupos diferentes (train/val vs test)\n",
    "    data.edge_index = filter_edges_by_split(\n",
    "        edge_index=data.edge_index,\n",
    "        train_idx=train_idx_run,\n",
    "        val_idx=val_idx_run,\n",
    "        test_idx=test_idx\n",
    "    ).to(data.x.device)\n",
    "\n",
    "    # Checagem \n",
    "    src, dst = data.edge_index.cpu().numpy()\n",
    "    train_val_set = set(train_idx_run.tolist() + val_idx_run.tolist())\n",
    "    test_set = set(test_idx.tolist())\n",
    "    cross_edges = [(s,d) for s,d in zip(src,dst) if (s in train_val_set and d in test_set) or (d in train_val_set and s in test_set)]\n",
    "    if len(cross_edges) > 0:\n",
    "        print(f\"ERRO: Existem {len(cross_edges)} arestas entre treino/val e teste! (seed {seed})\")\n",
    "\n",
    "    # Criar loaders\n",
    "    train_loader = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=[3,3],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        input_nodes=torch.tensor(train_idx_run),\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=[3,3],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        input_nodes=torch.tensor(val_idx_run),\n",
    "        shuffle=False\n",
    "    )\n",
    "    test_loader = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=[3,3],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        input_nodes=torch.tensor(test_idx),\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return scaler_local, train_loader, val_loader, test_loader, train_idx_run, val_idx_run, test_idx\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            outputs = model(batch).cpu().numpy()\n",
    "            targets = batch.y.cpu().numpy()\n",
    "            loss = criterion(torch.tensor(outputs), torch.tensor(targets))\n",
    "            total_loss += loss.item()\n",
    "            predictions.extend(outputs)\n",
    "            actuals.extend(targets)\n",
    "\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    mdae = median_absolute_error(actuals, predictions)\n",
    "    errors = np.abs(np.array(predictions) - np.array(actuals))\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        within_50 = np.where(np.array(actuals) != 0, errors <= (0.50 * np.array(actuals)), errors == 0)\n",
    "    pred_50 = float(np.mean(within_50))\n",
    "\n",
    "    return total_loss / max(1, len(dataloader)), float(mae), float(mse), float(mdae), float(pred_50)\n",
    "\n",
    "# Loop de N runs (cross-project) \n",
    "all_run_results = []\n",
    "print(f\"Iniciando {N_RUNS} runs (cross-project) - test project: {project_name} (ID {PROJECT_ID})\")\n",
    "\n",
    "for run_id in range(N_RUNS):\n",
    "    run_seed = SEED + run_id\n",
    "    set_all_seeds(run_seed)\n",
    "\n",
    "    # preparar split/loaders (restaurando edge_index original)\n",
    "    scaler_run, train_loader, val_loader, test_loader, train_idx_run, val_idx_run, test_idx_run = prepare_split_and_loaders_cross(run_seed, edge_index_orig)\n",
    "\n",
    "    # Instanciar modelo, optimizer, scheduler\n",
    "    in_channels = data.x.size(1)\n",
    "    hidden_dim = 64\n",
    "    model = SAGEModel(in_channels, hidden_dim).to(DEVICE)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=(PATIENCE//2)+1, factor=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_mse_history = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n--- Run {run_id+1}/{N_RUNS} (seed={run_seed}) ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch)\n",
    "            targets = batch.y.to(DEVICE)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / max(1, len(train_loader))\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        _, _, val_mse, *_ = evaluate_model(model, val_loader, criterion)\n",
    "        val_mse_history.append(val_mse)\n",
    "\n",
    "        print(f\"\\rRun {run_id+1}/{N_RUNS} Epoch {epoch+1}/{EPOCHS} - Train Loss: {avg_train_loss:.6f} - Val MSE: {val_mse:.6f}\", end=\"\", flush=True)\n",
    "\n",
    "        # early stopping\n",
    "        if val_mse < best_val_loss:\n",
    "            best_val_loss = val_mse\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        scheduler.step(val_mse)\n",
    "\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"\\nEarly stopping (run {run_id+1}) after epoch {epoch+1}. Best Val MSE: {best_val_loss:.6f}\")\n",
    "            break\n",
    "\n",
    "    # restaura melhor modelo\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # avaliação final no teste\n",
    "    _, mae_scaled, mse_scaled, mdae_scaled, pred50_scaled = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "    # converter métricas para escala real \n",
    "    if scaler_run is not None:\n",
    "        if NORMALIZE_TYPE == \"minmax\":\n",
    "            SP_MIN, SP_MAX = df_test[\"Story_Point\"].min(), df_test[\"Story_Point\"].max()\n",
    "            scale_factor = SP_MAX - SP_MIN\n",
    "            if scale_factor == 0:\n",
    "                scale_factor = 1.0\n",
    "            test_metrics_real = {\n",
    "                \"MAE\": mae_scaled * scale_factor,\n",
    "                \"MSE\": mse_scaled * (scale_factor ** 2),\n",
    "                \"MdAE\": mdae_scaled * scale_factor,\n",
    "                \"Pred(50)\": pred50_scaled\n",
    "            }\n",
    "        elif NORMALIZE_TYPE == \"standard\":\n",
    "            std_dev_real = df_test[\"Story_Point\"].std()\n",
    "            if std_dev_real == 0:\n",
    "                std_dev_real = 1.0\n",
    "            test_metrics_real = {\n",
    "                \"MAE\": mae_scaled * std_dev_real,\n",
    "                \"MSE\": mse_scaled * (std_dev_real ** 2),\n",
    "                \"MdAE\": mdae_scaled * std_dev_real,\n",
    "                \"Pred(50)\": pred50_scaled\n",
    "            }\n",
    "        else:\n",
    "            test_metrics_real = {\"MAE\": mae_scaled, \"MSE\": mse_scaled, \"MdAE\": mdae_scaled, \"Pred(50)\": pred50_scaled}\n",
    "    else:\n",
    "        test_metrics_real = {\"MAE\": mae_scaled, \"MSE\": mse_scaled, \"MdAE\": mdae_scaled, \"Pred(50)\": pred50_scaled}\n",
    "\n",
    "    # salvar resultados da run\n",
    "    run_result = {\n",
    "        \"run_id\": run_id + 1,\n",
    "        \"seed\": run_seed,\n",
    "        \"elapsed_sec\": elapsed,\n",
    "        \"best_val_mse\": float(best_val_loss),\n",
    "        \"epochs_trained\": epoch+1,\n",
    "        \"train_loss_history\": train_losses,\n",
    "        \"val_mse_history\": val_mse_history,\n",
    "        \"test_metrics_scaled\": {\"MAE\": float(mae_scaled), \"MSE\": float(mse_scaled), \"MdAE\": float(mdae_scaled), \"Pred(50)\": float(pred50_scaled)},\n",
    "        \"test_metrics_real\": {k: float(v) for k, v in test_metrics_real.items()}\n",
    "    }\n",
    "\n",
    "    # salvar arquivos\n",
    "    json_path = os.path.join(OUT_DIR, f\"{VERSAO_MODELO}_run{run_id+1}_seed{run_seed}.json\")\n",
    "    torch_path = os.path.join(OUT_DIR, f\"{VERSAO_MODELO}_run{run_id+1}_seed{run_seed}.pt\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(run_result, f, indent=4)\n",
    "    torch.save(best_model_state, torch_path)\n",
    "\n",
    "    all_run_results.append(run_result)\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# --- Consolidação final ---\n",
    "maes = [r[\"test_metrics_real\"][\"MAE\"] for r in all_run_results]\n",
    "mses = [r[\"test_metrics_real\"][\"MSE\"] for r in all_run_results]\n",
    "mdaes = [r[\"test_metrics_real\"][\"MdAE\"] for r in all_run_results]\n",
    "pred50s = [r[\"test_metrics_real\"][\"Pred(50)\"] for r in all_run_results]\n",
    "\n",
    "summary = {\n",
    "    \"n_runs\": N_RUNS,\n",
    "    \"seed_base\": SEED,\n",
    "    \"MAE_mean\": float(statistics.mean(maes)),\n",
    "    \"MAE_std\": float(statistics.pstdev(maes)),\n",
    "    \"MSE_mean\": float(statistics.mean(mses)),\n",
    "    \"MSE_std\": float(statistics.pstdev(mses)),\n",
    "    \"MdAE_mean\": float(statistics.mean(mdaes)),\n",
    "    \"MdAE_std\": float(statistics.pstdev(mdaes)),\n",
    "    \"Pred50_mean\": float(statistics.mean(pred50s)),\n",
    "    \"Pred50_std\": float(statistics.pstdev(pred50s)),\n",
    "    \"per_run_files\": [os.path.basename(f\"{VERSAO_MODELO}_run{r['run_id']}_seed{r['seed']}.json\") for r in all_run_results]\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(OUT_DIR, f\"{VERSAO_MODELO}_summary_{N_RUNS}runs.json\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump({\"summary\": summary, \"runs\": all_run_results}, f, indent=4)\n",
    "\n",
    "print(\"\\nAll cross-project runs finished.\")\n",
    "print(\"Summary saved to:\", summary_path)\n",
    "print(\"MAE mean (real scale):\", summary[\"MAE_mean\"], \"±\", summary[\"MAE_std\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Docker)",
   "language": "python",
   "name": "python_docker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
