{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel,BertPreTrainedModel,BertTokenizer,AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch.nn as nn\n",
    "global EPOCHS, BATCH_SIZE_RATIO, SEQUENCE_LEN, LEARNING_RATE, TOKENIZER, MODEL_NAME\n",
    "import torch.nn.functional as F\n",
    "from openpyxl import load_workbook\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for Project 4...\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 2.69\n",
      "-\n",
      " Average eval MAE loss: 1.95\n",
      "===============================\n",
      "MAE:  [2.381991]\n",
      "MdAE:  1.8187292\n",
      "MMRE:  [0.8229866]\n",
      "PRED:  0.0\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 2.60\n",
      "-\n",
      " Average eval MAE loss: 1.82\n",
      "===============================\n",
      "MAE:  [2.2486167]\n",
      "MdAE:  1.6744025\n",
      "MMRE:  [0.7507763]\n",
      "PRED:  0.109375\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 2.48\n",
      "-\n",
      " Average eval MAE loss: 1.64\n",
      "===============================\n",
      "MAE:  [2.0821242]\n",
      "MdAE:  1.4803218\n",
      "MMRE:  [0.6606339]\n",
      "PRED:  0.271875\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 2.29\n",
      "-\n",
      " Average eval MAE loss: 1.46\n",
      "===============================\n",
      "MAE:  [1.9000305]\n",
      "MdAE:  1.2934813\n",
      "MMRE:  [0.56200194]\n",
      "PRED:  0.29375\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 2.12\n",
      "-\n",
      " Average eval MAE loss: 1.28\n",
      "===============================\n",
      "MAE:  [1.7287061]\n",
      "MdAE:  1.1085978\n",
      "MMRE:  [0.4761893]\n",
      "PRED:  0.45\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 2.01\n",
      "-\n",
      " Average eval MAE loss: 1.17\n",
      "===============================\n",
      "MAE:  [1.6310387]\n",
      "MdAE:  0.93418944\n",
      "MMRE:  [0.46537656]\n",
      "PRED:  0.525\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 1.88\n",
      "-\n",
      " Average eval MAE loss: 1.13\n",
      "===============================\n",
      "MAE:  [1.5532705]\n",
      "MdAE:  0.7494075\n",
      "MMRE:  [0.47390142]\n",
      "PRED:  0.53125\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.72\n",
      "-\n",
      " Average eval MAE loss: 1.10\n",
      "===============================\n",
      "MAE:  [1.4781933]\n",
      "MdAE:  0.6713058\n",
      "MMRE:  [0.48220548]\n",
      "PRED:  0.528125\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 1.67\n",
      "-\n",
      " Average eval MAE loss: 1.07\n",
      "===============================\n",
      "MAE:  [1.4065902]\n",
      "MdAE:  0.84356695\n",
      "MMRE:  [0.49021173]\n",
      "PRED:  0.534375\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.51\n",
      "-\n",
      " Average eval MAE loss: 1.05\n",
      "===============================\n",
      "MAE:  [1.3432522]\n",
      "MdAE:  0.9715048\n",
      "MMRE:  [0.5001081]\n",
      "PRED:  0.534375\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.45\n",
      "-\n",
      " Average eval MAE loss: 1.05\n",
      "===============================\n",
      "MAE:  [1.3131688]\n",
      "MdAE:  0.97847223\n",
      "MMRE:  [0.52400887]\n",
      "PRED:  0.534375\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 1.39\n",
      "-\n",
      " Average eval MAE loss: 1.08\n",
      "===============================\n",
      "MAE:  [1.3112338]\n",
      "MdAE:  0.9608364\n",
      "MMRE:  [0.55887586]\n",
      "PRED:  0.540625\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 1.39\n",
      "-\n",
      " Average eval MAE loss: 1.12\n",
      "===============================\n",
      "MAE:  [1.31653]\n",
      "MdAE:  0.8546649\n",
      "MMRE:  [0.5936705]\n",
      "PRED:  0.55\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 1.35\n",
      "-\n",
      " Average eval MAE loss: 1.15\n",
      "===============================\n",
      "MAE:  [1.3220526]\n",
      "MdAE:  0.7492856\n",
      "MMRE:  [0.62464917]\n",
      "PRED:  0.571875\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 1.34\n",
      "-\n",
      " Average eval MAE loss: 1.18\n",
      "===============================\n",
      "MAE:  [1.3265016]\n",
      "MdAE:  0.71448207\n",
      "MMRE:  [0.65062165]\n",
      "PRED:  0.584375\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 1.33\n",
      "-\n",
      " Average eval MAE loss: 1.20\n",
      "===============================\n",
      "MAE:  [1.3298019]\n",
      "MdAE:  0.7402246\n",
      "MMRE:  [0.67093444]\n",
      "PRED:  0.609375\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 1.31\n",
      "-\n",
      " Average eval MAE loss: 1.22\n",
      "===============================\n",
      "MAE:  [1.3328055]\n",
      "MdAE:  0.79979074\n",
      "MMRE:  [0.6868204]\n",
      "PRED:  0.625\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 1.34\n",
      "-\n",
      " Average eval MAE loss: 1.23\n",
      "===============================\n",
      "MAE:  [1.3353244]\n",
      "MdAE:  0.82350624\n",
      "MMRE:  [0.6987227]\n",
      "PRED:  0.63125\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 1.28\n",
      "-\n",
      " Average eval MAE loss: 1.24\n",
      "===============================\n",
      "MAE:  [1.3368689]\n",
      "MdAE:  0.8495258\n",
      "MMRE:  [0.7056178]\n",
      "PRED:  0.6375\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 1.29\n",
      "-\n",
      " Average eval MAE loss: 1.24\n",
      "===============================\n",
      "MAE:  [1.3373864]\n",
      "MdAE:  0.8581741\n",
      "MMRE:  [0.7078898]\n",
      "PRED:  0.634375\n",
      "all done for one project\n",
      "results have been written into a text file!\n",
      "within project split!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for Project 12...\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 4.86\n",
      "-\n",
      " Average eval MAE loss: 6.81\n",
      "===============================\n",
      "MAE:  [6.043938]\n",
      "MdAE:  4.8430643\n",
      "MMRE:  [0.9596383]\n",
      "PRED:  0.0\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 4.63\n",
      "-\n",
      " Average eval MAE loss: 6.71\n",
      "===============================\n",
      "MAE:  [5.9479427]\n",
      "MdAE:  4.7323604\n",
      "MMRE:  [0.93051434]\n",
      "PRED:  0.003738317757009346\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 4.63\n",
      "-\n",
      " Average eval MAE loss: 6.59\n",
      "===============================\n",
      "MAE:  [5.8457665]\n",
      "MdAE:  4.617207\n",
      "MMRE:  [0.8993338]\n",
      "PRED:  0.026168224299065422\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 4.44\n",
      "-\n",
      " Average eval MAE loss: 6.48\n",
      "===============================\n",
      "MAE:  [5.741067]\n",
      "MdAE:  4.5003805\n",
      "MMRE:  [0.8673422]\n",
      "PRED:  0.03551401869158879\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 4.38\n",
      "-\n",
      " Average eval MAE loss: 6.36\n",
      "===============================\n",
      "MAE:  [5.634415]\n",
      "MdAE:  4.383033\n",
      "MMRE:  [0.83834463]\n",
      "PRED:  0.09532710280373832\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 4.26\n",
      "-\n",
      " Average eval MAE loss: 6.24\n",
      "===============================\n",
      "MAE:  [5.5295634]\n",
      "MdAE:  4.2625732\n",
      "MMRE:  [0.81627303]\n",
      "PRED:  0.09906542056074766\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 4.17\n",
      "-\n",
      " Average eval MAE loss: 6.12\n",
      "===============================\n",
      "MAE:  [5.4239063]\n",
      "MdAE:  4.1376367\n",
      "MMRE:  [0.79398274]\n",
      "PRED:  0.08224299065420561\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 4.03\n",
      "-\n",
      " Average eval MAE loss: 6.00\n",
      "===============================\n",
      "MAE:  [5.3186913]\n",
      "MdAE:  4.0181537\n",
      "MMRE:  [0.7720667]\n",
      "PRED:  0.07476635514018691\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 3.90\n",
      "-\n",
      " Average eval MAE loss: 5.89\n",
      "===============================\n",
      "MAE:  [5.218957]\n",
      "MdAE:  3.8965766\n",
      "MMRE:  [0.75490916]\n",
      "PRED:  0.11401869158878504\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 3.81\n",
      "-\n",
      " Average eval MAE loss: 5.78\n",
      "===============================\n",
      "MAE:  [5.130236]\n",
      "MdAE:  3.7797456\n",
      "MMRE:  [0.747253]\n",
      "PRED:  0.16261682242990655\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 3.74\n",
      "-\n",
      " Average eval MAE loss: 5.68\n",
      "===============================\n",
      "MAE:  [5.0484166]\n",
      "MdAE:  3.6725214\n",
      "MMRE:  [0.74236906]\n",
      "PRED:  0.16635514018691588\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 3.58\n",
      "-\n",
      " Average eval MAE loss: 5.59\n",
      "===============================\n",
      "MAE:  [4.972335]\n",
      "MdAE:  3.5725422\n",
      "MMRE:  [0.737933]\n",
      "PRED:  0.17570093457943925\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 3.57\n",
      "-\n",
      " Average eval MAE loss: 5.50\n",
      "===============================\n",
      "MAE:  [4.9026985]\n",
      "MdAE:  3.480697\n",
      "MMRE:  [0.73388356]\n",
      "PRED:  0.21495327102803738\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 3.52\n",
      "-\n",
      " Average eval MAE loss: 5.42\n",
      "===============================\n",
      "MAE:  [4.8402934]\n",
      "MdAE:  3.3963277\n",
      "MMRE:  [0.730244]\n",
      "PRED:  0.24485981308411214\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 3.40\n",
      "-\n",
      " Average eval MAE loss: 5.36\n",
      "===============================\n",
      "MAE:  [4.7858534]\n",
      "MdAE:  3.3226688\n",
      "MMRE:  [0.7270708]\n",
      "PRED:  0.2672897196261682\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 3.41\n",
      "-\n",
      " Average eval MAE loss: 5.30\n",
      "===============================\n",
      "MAE:  [4.740043]\n",
      "MdAE:  3.2622724\n",
      "MMRE:  [0.7243996]\n",
      "PRED:  0.2841121495327103\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 3.32\n",
      "-\n",
      " Average eval MAE loss: 5.26\n",
      "===============================\n",
      "MAE:  [4.703451]\n",
      "MdAE:  3.2145944\n",
      "MMRE:  [0.7222631]\n",
      "PRED:  0.2841121495327103\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 3.28\n",
      "-\n",
      " Average eval MAE loss: 5.22\n",
      "===============================\n",
      "MAE:  [4.676574]\n",
      "MdAE:  3.1795273\n",
      "MMRE:  [0.72069263]\n",
      "PRED:  0.2841121495327103\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 3.22\n",
      "-\n",
      " Average eval MAE loss: 5.20\n",
      "===============================\n",
      "MAE:  [4.6597643]\n",
      "MdAE:  3.1575737\n",
      "MMRE:  [0.71970963]\n",
      "PRED:  0.2841121495327103\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 3.28\n",
      "-\n",
      " Average eval MAE loss: 5.19\n",
      "===============================\n",
      "MAE:  [4.6532555]\n",
      "MdAE:  3.1490784\n",
      "MMRE:  [0.7193292]\n",
      "PRED:  0.2841121495327103\n",
      "all done for one project\n",
      "results have been written into a text file!\n",
      "within project split!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for Project 43...\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 1.85\n",
      "-\n",
      " Average eval MAE loss: 2.04\n",
      "===============================\n",
      "MAE:  [1.720751]\n",
      "MdAE:  1.1610298\n",
      "MMRE:  [1.113127]\n",
      "PRED:  0.0\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 1.77\n",
      "-\n",
      " Average eval MAE loss: 1.99\n",
      "===============================\n",
      "MAE:  [1.6713607]\n",
      "MdAE:  1.1078936\n",
      "MMRE:  [1.0746052]\n",
      "PRED:  0.0\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 1.70\n",
      "-\n",
      " Average eval MAE loss: 1.94\n",
      "===============================\n",
      "MAE:  [1.6179519]\n",
      "MdAE:  1.050791\n",
      "MMRE:  [1.0327698]\n",
      "PRED:  0.0\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 1.68\n",
      "-\n",
      " Average eval MAE loss: 1.90\n",
      "===============================\n",
      "MAE:  [1.5651531]\n",
      "MdAE:  1.0012418\n",
      "MMRE:  [0.99164796]\n",
      "PRED:  0.0\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 1.62\n",
      "-\n",
      " Average eval MAE loss: 1.85\n",
      "===============================\n",
      "MAE:  [1.5212239]\n",
      "MdAE:  0.95731026\n",
      "MMRE:  [0.9576415]\n",
      "PRED:  0.0\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 1.60\n",
      "-\n",
      " Average eval MAE loss: 1.81\n",
      "===============================\n",
      "MAE:  [1.4758015]\n",
      "MdAE:  0.9118146\n",
      "MMRE:  [0.92233163]\n",
      "PRED:  0.0\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 1.56\n",
      "-\n",
      " Average eval MAE loss: 1.77\n",
      "===============================\n",
      "MAE:  [1.4307456]\n",
      "MdAE:  0.87223244\n",
      "MMRE:  [0.8873351]\n",
      "PRED:  0.0\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.53\n",
      "-\n",
      " Average eval MAE loss: 1.72\n",
      "===============================\n",
      "MAE:  [1.3762896]\n",
      "MdAE:  0.8252178\n",
      "MMRE:  [0.8451279]\n",
      "PRED:  0.0\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 1.50\n",
      "-\n",
      " Average eval MAE loss: 1.66\n",
      "===============================\n",
      "MAE:  [1.3075391]\n",
      "MdAE:  0.76171875\n",
      "MMRE:  [0.7919266]\n",
      "PRED:  0.0\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.43\n",
      "-\n",
      " Average eval MAE loss: 1.58\n",
      "===============================\n",
      "MAE:  [1.2280841]\n",
      "MdAE:  0.68749887\n",
      "MMRE:  [0.7304712]\n",
      "PRED:  0.0\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.38\n",
      "-\n",
      " Average eval MAE loss: 1.50\n",
      "===============================\n",
      "MAE:  [1.144877]\n",
      "MdAE:  0.60607207\n",
      "MMRE:  [0.66605943]\n",
      "PRED:  0.059130434782608696\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 1.31\n",
      "-\n",
      " Average eval MAE loss: 1.43\n",
      "===============================\n",
      "MAE:  [1.0660133]\n",
      "MdAE:  0.53283036\n",
      "MMRE:  [0.60500795]\n",
      "PRED:  0.391304347826087\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 1.25\n",
      "-\n",
      " Average eval MAE loss: 1.36\n",
      "===============================\n",
      "MAE:  [0.9937279]\n",
      "MdAE:  0.463848\n",
      "MMRE:  [0.54909825]\n",
      "PRED:  0.5652173913043478\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 1.22\n",
      "-\n",
      " Average eval MAE loss: 1.30\n",
      "===============================\n",
      "MAE:  [0.9312959]\n",
      "MdAE:  0.39725375\n",
      "MMRE:  [0.5008522]\n",
      "PRED:  0.5895652173913043\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 1.16\n",
      "-\n",
      " Average eval MAE loss: 1.25\n",
      "===============================\n",
      "MAE:  [0.8787069]\n",
      "MdAE:  0.34512907\n",
      "MMRE:  [0.4602195]\n",
      "PRED:  0.591304347826087\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 1.13\n",
      "-\n",
      " Average eval MAE loss: 1.20\n",
      "===============================\n",
      "MAE:  [0.8351485]\n",
      "MdAE:  0.30280328\n",
      "MMRE:  [0.42655793]\n",
      "PRED:  0.591304347826087\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 1.09\n",
      "-\n",
      " Average eval MAE loss: 1.17\n",
      "===============================\n",
      "MAE:  [0.8007113]\n",
      "MdAE:  0.26904523\n",
      "MMRE:  [0.39993334]\n",
      "PRED:  0.591304347826087\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 1.05\n",
      "-\n",
      " Average eval MAE loss: 1.15\n",
      "===============================\n",
      "MAE:  [0.77557373]\n",
      "MdAE:  0.24568015\n",
      "MMRE:  [0.38049477]\n",
      "PRED:  0.591304347826087\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 1.05\n",
      "-\n",
      " Average eval MAE loss: 1.13\n",
      "===============================\n",
      "MAE:  [0.7599149]\n",
      "MdAE:  0.23062712\n",
      "MMRE:  [0.36838552]\n",
      "PRED:  0.591304347826087\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 1.02\n",
      "-\n",
      " Average eval MAE loss: 1.13\n",
      "===============================\n",
      "MAE:  [0.7538859]\n",
      "MdAE:  0.22464728\n",
      "MMRE:  [0.36372328]\n",
      "PRED:  0.591304347826087\n",
      "all done for one project\n",
      "results have been written into a text file!\n",
      "within project split!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for Project 36...\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 4.21\n",
      "-\n",
      " Average eval MAE loss: 3.68\n",
      "===============================\n",
      "MAE:  [4.437868]\n",
      "MdAE:  3.057982\n",
      "MMRE:  [1.0064965]\n",
      "PRED:  0.0\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 4.15\n",
      "-\n",
      " Average eval MAE loss: 3.66\n",
      "===============================\n",
      "MAE:  [4.411981]\n",
      "MdAE:  3.0503898\n",
      "MMRE:  [0.9982751]\n",
      "PRED:  0.0\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 4.03\n",
      "-\n",
      " Average eval MAE loss: 3.64\n",
      "===============================\n",
      "MAE:  [4.387118]\n",
      "MdAE:  3.0242815\n",
      "MMRE:  [0.99026084]\n",
      "PRED:  0.0\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 4.12\n",
      "-\n",
      " Average eval MAE loss: 3.63\n",
      "===============================\n",
      "MAE:  [4.3649774]\n",
      "MdAE:  2.9994357\n",
      "MMRE:  [0.98299366]\n",
      "PRED:  0.0\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 4.04\n",
      "-\n",
      " Average eval MAE loss: 3.61\n",
      "===============================\n",
      "MAE:  [4.3426147]\n",
      "MdAE:  2.978831\n",
      "MMRE:  [0.97596717]\n",
      "PRED:  0.0\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 4.09\n",
      "-\n",
      " Average eval MAE loss: 3.60\n",
      "===============================\n",
      "MAE:  [4.3194556]\n",
      "MdAE:  2.9601655\n",
      "MMRE:  [0.9686869]\n",
      "PRED:  0.0\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 4.01\n",
      "-\n",
      " Average eval MAE loss: 3.58\n",
      "===============================\n",
      "MAE:  [4.2982674]\n",
      "MdAE:  2.9396477\n",
      "MMRE:  [0.96201444]\n",
      "PRED:  0.0\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 4.05\n",
      "-\n",
      " Average eval MAE loss: 3.57\n",
      "===============================\n",
      "MAE:  [4.278338]\n",
      "MdAE:  2.9155881\n",
      "MMRE:  [0.955987]\n",
      "PRED:  0.0\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 3.98\n",
      "-\n",
      " Average eval MAE loss: 3.55\n",
      "===============================\n",
      "MAE:  [4.259562]\n",
      "MdAE:  2.8913863\n",
      "MMRE:  [0.95037746]\n",
      "PRED:  0.0\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 3.99\n",
      "-\n",
      " Average eval MAE loss: 3.54\n",
      "===============================\n",
      "MAE:  [4.241312]\n",
      "MdAE:  2.868073\n",
      "MMRE:  [0.94486]\n",
      "PRED:  0.0\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 4.05\n",
      "-\n",
      " Average eval MAE loss: 3.53\n",
      "===============================\n",
      "MAE:  [4.2234974]\n",
      "MdAE:  2.8465867\n",
      "MMRE:  [0.93949157]\n",
      "PRED:  0.0\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 3.97\n",
      "-\n",
      " Average eval MAE loss: 3.51\n",
      "===============================\n",
      "MAE:  [4.2060127]\n",
      "MdAE:  2.8260574\n",
      "MMRE:  [0.9342866]\n",
      "PRED:  0.0\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 3.97\n",
      "-\n",
      " Average eval MAE loss: 3.50\n",
      "===============================\n",
      "MAE:  [4.1884747]\n",
      "MdAE:  2.8061612\n",
      "MMRE:  [0.92910147]\n",
      "PRED:  0.0\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 3.93\n",
      "-\n",
      " Average eval MAE loss: 3.49\n",
      "===============================\n",
      "MAE:  [4.1709085]\n",
      "MdAE:  2.7859778\n",
      "MMRE:  [0.92390746]\n",
      "PRED:  0.0\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 3.89\n",
      "-\n",
      " Average eval MAE loss: 3.48\n",
      "===============================\n",
      "MAE:  [4.1537766]\n",
      "MdAE:  2.7661514\n",
      "MMRE:  [0.91884625]\n",
      "PRED:  0.0\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 3.80\n",
      "-\n",
      " Average eval MAE loss: 3.47\n",
      "===============================\n",
      "MAE:  [4.1377997]\n",
      "MdAE:  2.7477288\n",
      "MMRE:  [0.91412663]\n",
      "PRED:  0.0\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 3.89\n",
      "-\n",
      " Average eval MAE loss: 3.46\n",
      "===============================\n",
      "MAE:  [4.124009]\n",
      "MdAE:  2.7317934\n",
      "MMRE:  [0.9100454]\n",
      "PRED:  0.0\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 3.83\n",
      "-\n",
      " Average eval MAE loss: 3.45\n",
      "===============================\n",
      "MAE:  [4.1133285]\n",
      "MdAE:  2.7191348\n",
      "MMRE:  [0.90688264]\n",
      "PRED:  0.0\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 3.88\n",
      "-\n",
      " Average eval MAE loss: 3.44\n",
      "===============================\n",
      "MAE:  [4.1064386]\n",
      "MdAE:  2.710855\n",
      "MMRE:  [0.9048425]\n",
      "PRED:  0.0\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 3.80\n",
      "-\n",
      " Average eval MAE loss: 3.44\n",
      "===============================\n",
      "MAE:  [4.103734]\n",
      "MdAE:  2.7075753\n",
      "MMRE:  [0.9040414]\n",
      "PRED:  0.0\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    }
   ],
   "source": [
    "projectnum = 2 # serve apenas pra escrever em open.xlsx que tem os resultados comparativos entre projetos\n",
    "ROW_MAE, ROW_MMRE, ROW_PRED = 3,4,5\n",
    "\n",
    "RESULT_FILE_PATH = r'./open.xlsx'\n",
    "\n",
    "\n",
    "\n",
    "wb = load_workbook(RESULT_FILE_PATH)\n",
    "sheet = wb.active\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE_RATIO = 0.3\n",
    "SEQUENCE_LEN = 20\n",
    "LEARNING_RATE = 5e-4\n",
    "# define device\n",
    "global DEVICE\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# define files to be used\n",
    "DATA_FILE = './open/issues_sp_filtered_with_comments.csv'  \n",
    "\n",
    "\n",
    "\n",
    "# projetos da nova base\n",
    "TRAIN_PROJECTS = [4, 12, 43, 36] \n",
    "\n",
    "\n",
    "OUTPUT = ''\n",
    "MODEL = None\n",
    "DYNAMIC_BATCH = True\n",
    "BATCH_SIZE = None\n",
    "WITHIN_PROJECT = None\n",
    "MAE_RECORDS = []\n",
    "MDAE_RECORDS = []\n",
    "\n",
    "def data_processing(project_data):\n",
    "    global BATCH_SIZE, BATCH_SIZE_RATIO, WITHIN_PROJECT, DYNAMIC_BATCH\n",
    "\n",
    "    \n",
    "    # Garantir que os dados não estejam vazios\n",
    "    if project_data.empty:\n",
    "        print(\"Nenhum dado encontrado para o projeto.\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "    train_data = project_data.copy()  \n",
    "\n",
    "        \n",
    "    # data split\n",
    "    if WITHIN_PROJECT:\n",
    "        train_ex,train_text,train_labels,val_ex,val_text, val_labels, test_ex,test_text, test_labels = within_project_split(train_data)\n",
    "    # define batch size dynamicalloutputsy based on training length\n",
    "    if DYNAMIC_BATCH:\n",
    "        BATCH_SIZE = int(len(train_text) * BATCH_SIZE_RATIO)\n",
    "    # tokenization\n",
    "    tokens_train = tokenization(train_text.tolist())\n",
    "    tokens_val = tokenization(val_text.tolist())\n",
    " \n",
    "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "    train_ex = np.array(train_ex)\n",
    "    train_ex = torch.tensor(train_ex)\n",
    "    train_y = torch.tensor(train_labels.tolist()).type(torch.FloatTensor)\n",
    "    train_seq = torch.cat((train_ex,train_seq),dim=1)\n",
    "    train_dataloader = prepare_dataloader(train_seq, train_y, sampler_type='random')\n",
    "\n",
    "    val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "    val_ex = np.array(val_ex)\n",
    "    val_ex = torch.tensor(val_ex)\n",
    "    val_y = torch.tensor(val_labels.tolist()).type(torch.FloatTensor)\n",
    "    val_seq = torch.cat((val_ex,val_seq),dim=1)\n",
    "    val_dataloader = prepare_dataloader(val_seq, val_y, sampler_type='sequential')\n",
    "    \n",
    "    # prepare testing datasets\n",
    "    all_test_dataloader = []\n",
    "    # test_file_names = []\n",
    "    if WITHIN_PROJECT:\n",
    "        tokens_test = tokenization(test_text.tolist())\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_ex = np.array(test_ex)\n",
    "        test_ex = torch.tensor(test_ex)\n",
    "        test_seq = torch.cat((test_ex,test_seq),dim=1)\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.FloatTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "        all_test_dataloader.append(test_dataloader)\n",
    "        # test_file_names.append(file_pair)\n",
    "        return train_dataloader, val_dataloader, all_test_dataloader\n",
    "\n",
    "\n",
    "def tokenization(text_list):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    return tokenizer(text_list, truncation=True, max_length=SEQUENCE_LEN, padding='max_length')\n",
    "\n",
    "\n",
    "def prepare_dataframe(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    # order=['Assignee_count','Reporter_count','Creator_count','Summary','Custom field (Story Points)']\n",
    "    order = ['Project_ID','Reputation_t', 'Title', 'Story_Point']\n",
    "\n",
    "    data=data[order]\n",
    "    data = data.fillna(0)\n",
    "    return pd.DataFrame(data=data)\n",
    "\n",
    "\n",
    "def prepare_dataloader(seq, y, sampler_type):\n",
    "    global BATCH_SIZE\n",
    "    tensor_dataset = TensorDataset(seq, y)\n",
    "    if sampler_type == 'random':\n",
    "        sampler = RandomSampler(tensor_dataset)\n",
    "    elif sampler_type == 'sequential':\n",
    "        sampler = SequentialSampler(tensor_dataset)\n",
    "    dataloader = DataLoader(tensor_dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def within_project_split(data):\n",
    "    print('within project split!')\n",
    "    train_val_split_point = int(len(data) * 0.6)\n",
    "    val_test_split_point = int(len(data) * 0.8)\n",
    "    train_ex=data.iloc[:train_val_split_point,0:1]\n",
    "    train_text = data['Title'][:train_val_split_point]\n",
    "    train_labels = (data['Story_Point'][:train_val_split_point])\n",
    "    val_ex=data.iloc[train_val_split_point:val_test_split_point,0:1]\n",
    "    val_text = data['Title'][train_val_split_point:val_test_split_point]\n",
    "    val_labels = (data['Story_Point'][train_val_split_point:val_test_split_point])\n",
    "    test_ex=data.iloc[val_test_split_point:,0:1]\n",
    "    test_text = data['Title'][val_test_split_point:]\n",
    "    test_labels = (data['Story_Point'][val_test_split_point:])\n",
    "    return train_ex,train_text,train_labels,val_ex,val_text, val_labels, test_ex,test_text, test_labels\n",
    "\n",
    "class BertForSequence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertForSequence, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "        for name, param in self.bert.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        self.hidden1=nn.Linear(768, 3)\n",
    "        self.hidden2=nn.Linear(4,50)\n",
    "        self.score = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        outputs_bert = self.bert(input_ids[:,1:].long(), token_type_ids, attention_mask)\n",
    "        outputs = outputs_bert.last_hidden_state[:,0,:]\n",
    "        outputs=self.hidden1(outputs)\n",
    "        outputs=torch.cat((input_ids[:,0:1],outputs),dim=1)\n",
    "        outputs=torch.relu(self.hidden2(outputs.float()))\n",
    "        logit = self.score(outputs)\n",
    "        return logit\n",
    "\n",
    "def train_eval_test(project_id, train_dataloader, val_dataloader, all_test_dataloader, model):\n",
    "    global LEARNING_RATE, EPOCHS, MAE_RECORDS, MDAE_RECORDS, DEVICE\n",
    "    optimizer = AdamW(MODEL.parameters(), lr=LEARNING_RATE)    \n",
    "    # Total number of training steps is [number of batches] x [number of epochs]\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    # Create the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    print(f\"Start training for Project {project_id}...\")\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "\n",
    "    min_eval_loss_epoch = [10000, 0]\n",
    "    \n",
    "    time_records = []\n",
    "    MAE_RECORDS = []\n",
    "    MDAE_RECORDS = []\n",
    "    MMRE_RECORDS = []\n",
    "    PRED_RECPRDS=[]\n",
    "    start_time = time.time()\n",
    "    loss_fct = nn.L1Loss()\n",
    "    for e in range(EPOCHS):\n",
    "        # ---TRAINING---\n",
    "        # clean GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\">>> epoch \", e)\n",
    "        # set model into train mode\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):            \n",
    "            b_input_ids = batch[0].to(DEVICE)\n",
    "            b_labels = batch[1].to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            result = model(b_input_ids, \n",
    "                           labels=b_labels,\n",
    "                           )\n",
    "            # loss = loss_fct(result,b_labels)\n",
    "            loss = loss_fct(result.view(-1), b_labels)\n",
    "\n",
    "            logits=result\n",
    "            total_train_loss += loss.item()  \n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # clean memory\n",
    "            del step, batch, b_input_ids, b_labels, result, loss, logits\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(\" Average training MAE loss: {0:.2f}\".format(avg_train_loss))\n",
    "        # clean memory\n",
    "        del avg_train_loss, total_train_loss\n",
    "        \n",
    "        time_records.append(time.time() - start_time)\n",
    "        \n",
    "        # ---EVAL---\n",
    "        print(\"-\")\n",
    "        # set model into eval mode\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        for batch in val_dataloader:            \n",
    "            b_input_ids = batch[0].to(DEVICE)\n",
    "            b_labels = batch[1].to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            result = model(b_input_ids, \n",
    "                           labels=b_labels,\n",
    "                           )\n",
    "            # loss = loss_fct(result,b_labels)\n",
    "            loss = loss_fct(result.view(-1), b_labels)\n",
    "\n",
    "            logits = result\n",
    "            total_eval_loss += loss.item()  \n",
    "            # clean memory\n",
    "            del b_input_ids, b_labels, batch, result, loss, logits\n",
    "        avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
    "        print(\" Average eval MAE loss: {0:.2f}\".format(avg_eval_loss))\n",
    "        \n",
    "        if avg_eval_loss <= min_eval_loss_epoch[0]:\n",
    "            min_eval_loss_epoch[0] = avg_eval_loss\n",
    "            min_eval_loss_epoch[1] = e\n",
    "        \n",
    "        # clean memory\n",
    "        del avg_eval_loss, total_eval_loss\n",
    "        # save model state to dict\n",
    "        \n",
    "        print(\"===============================\")\n",
    "        \n",
    "        # testing on holdout data\n",
    "        # index = 0\n",
    "        for test_dataloader in all_test_dataloader:\n",
    "            # test_file_name = test_file_names[index]\n",
    "            # index += 1\n",
    "            testing_start_time = time.time()\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            for batch in test_dataloader:\n",
    "                batch = tuple(t.to(DEVICE) for t in batch)\n",
    "                b_input_ids, b_labels = batch\n",
    "                with torch.no_grad():\n",
    "                    logits = model(b_input_ids)\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                predictions.append(logits)\n",
    "                true_labels.append(label_ids)\n",
    "            # calculate errors\n",
    "            total_distance = 0\n",
    "            total_mre = 0\n",
    "            m=0\n",
    "            distance_records = []\n",
    "            total_data_point=0\n",
    "            for i in range(len(predictions)):\n",
    "                total_data_point+=len(predictions[i])\n",
    "            for i in range(len(predictions)):\n",
    "                for j in range(len(predictions[i])):\n",
    "                    distance = abs(predictions[i][j] - true_labels[i][j])\n",
    "                    if(true_labels[i][j]>0):\n",
    "                        mre=abs(predictions[i][j] - true_labels[i][j])/true_labels[i][j]\n",
    "                    else:\n",
    "                        mre=(abs(predictions[i][j] - true_labels[i][j])+1)/(true_labels[i][j]+1)\n",
    "                    if mre<0.5:\n",
    "                        m+=1\n",
    "                    total_mre+=mre\n",
    "                    total_distance += distance\n",
    "                    distance_records.append(distance)\n",
    "            MAE = total_distance / total_data_point\n",
    "            MMRE= total_mre / total_data_point\n",
    "            MdAE = np.median(np.array(distance_records)) \n",
    "            PRED=m/total_data_point\n",
    "            MAE_RECORDS.append(MAE)\n",
    "            MDAE_RECORDS.append(MdAE)\n",
    "            MMRE_RECORDS.append(MMRE)\n",
    "            PRED_RECPRDS.append(PRED)\n",
    "            \n",
    "            global OUTPUT\n",
    "            OUTPUT +=  'Epochs ' + str(e) + '\\n'\n",
    "            OUTPUT += 'MAE: ' + str(MAE) + '\\n'\n",
    "            OUTPUT += 'MdAE: ' + str(MdAE) + '\\n'\n",
    "            OUTPUT += 'MMRE: ' + str(MMRE) + '\\n'\n",
    "            OUTPUT += 'PRED: ' + str(PRED) + '\\n\\n'\n",
    "            print('MAE: ', MAE)\n",
    "            print('MdAE: ', MdAE)\n",
    "            print('MMRE: ', MMRE)\n",
    "            print('PRED: ', PRED)\n",
    "    \n",
    "            \n",
    "    OUTPUT +=str(MAE_RECORDS[min_eval_loss_epoch[1]])  + '\\n'+str(MMRE_RECORDS[min_eval_loss_epoch[1]]) + '\\n'+ str(PRED_RECPRDS[min_eval_loss_epoch[1]]) + '\\n'\n",
    "    OUTPUT += 'training time: ' + str(time_records[min_eval_loss_epoch[1]]) + '\\n'\n",
    "    OUTPUT += 'Epochs: ' + str(min_eval_loss_epoch[1]) +'\\n'\n",
    "    global BATCH_SIZE\n",
    "    OUTPUT += 'batch size: ' + str(BATCH_SIZE)\n",
    "    print('all done for one project')\n",
    "    sheet.cell(row=ROW_MAE, column=projectnum).value = MAE_RECORDS[min_eval_loss_epoch[1]][0]\n",
    "    sheet.cell(row=ROW_MMRE, column=projectnum).value = MMRE_RECORDS[min_eval_loss_epoch[1]][0]\n",
    "    sheet.cell(row=ROW_PRED, column=projectnum).value = PRED_RECPRDS[min_eval_loss_epoch[1]]\n",
    "    wb.save(RESULT_FILE_PATH)\n",
    "\n",
    "WITHIN_PROJECT = True\n",
    "\n",
    "\n",
    "def main():\n",
    "    global  MODEL, TOKENIZER, MODEL_NAME\n",
    "  \n",
    "    # Carregar os dados \n",
    "    df = prepare_dataframe(DATA_FILE)\n",
    "    \n",
    "    for project_id in TRAIN_PROJECTS:\n",
    "        project_data = df[df['Project_ID'] == project_id]  # Filtrar dados do projeto\n",
    "\n",
    "        if project_data.empty:\n",
    "            print(f\"Projeto {project_id} não possui dados suficientes. Pulando...\")\n",
    "            continue\n",
    "\n",
    "        MODEL = BertForSequence().to(DEVICE)\n",
    "\n",
    "        train_dataloader, val_dataloader, all_test_dataloader = data_processing(project_data.drop(columns=['Project_ID']))\n",
    "\n",
    "        train_eval_test(project_id, train_dataloader, val_dataloader, all_test_dataloader, MODEL)\n",
    "\n",
    "        del MODEL\n",
    "        torch.cuda.empty_cache()   \n",
    "\n",
    "        global OUTPUT\n",
    "        with open('./result_bert/project_' + str(project_id) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "        global projectnum\n",
    "        projectnum=projectnum+1\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Docker)",
   "language": "python",
   "name": "python_docker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
